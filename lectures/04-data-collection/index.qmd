---
title: "Data Collection"
author: 
  - name: Stephanie Hicks
    url: https://stephaniehicks.com
    affiliation: Department of Biostatistics, Johns Hopkins
    affiliation_url: https://publichealth.jhu.edu
description: "Using APIs and extracting data from HTMLs"
date: 2024-11-07
---

# Pre-lecture activities

::: callout-important

In advance of class, please install the following R packages

1. `jsonlite` (<https://jeroen.cran.dev/jsonlite>)
2. `httr2` (<https://httr2.r-lib.org>)
3. `rvest` (<https://rvest.tidyverse.org>) (should be installed already with the `tidyverse`)

```{r}
#| eval: false
install.packages("jsonlite")
install.packages("httr2")
```

:::

In addition, please read through

- [Data collection with APIs](../../readings/04-data-collection-apis/index.qmd)
- [Data collection from HTMLs](../../readings/04-data-collection-html/index.qmd)


# Lecture 

## Acknowledgements

Material for this lecture was borrowed and adopted from

- <https://jeroen.cran.dev/jsonlite>
- <https://httr2.r-lib.org>
- <https://rvest.tidyverse.org>

## Learning objectives

::: callout-note
### Learning objectives

**At the end of this lesson you will:**

- Describe what the difference is between "raw" vs "clean" data
- Learn about what are JSON files and how we can convert them into data frames in R 
- Describe some best practices on sharing data with collaborators
- Know what does API mean and state four types of API architectures
- Practice with the GitHub API and make authenticated requests
- Practice a range of `rvest` functions to scrape data from HTML pages
- Recongize various HTML elements on the page (text, links, images, lists, etc.)

:::


## Slides 

- <add here>

# Class activity 

For the rest of the time in class, we will practice using `rvest` to learn how to scrape data from HTML pages. To do this, we will load the following packages. 

```{r}
#| message: false
library(tidyverse)
library(rvest) # should be installed with the tidyverse
library(xml2)
```


::: callout-note
### Objectives of the activity

- Practice a range of `rvest` functions to scrape data from the web 
- Understand various various HTML elements on the page (text, links, images, lists, etc.). 

:::

::: {.callout-tip}

If any tables are nested within specific sections, you may need to target them individually using CSS selectors.

:::



## Part 1: Extracting tables 

- Use `rvest` to extract a table containing the World Cup champions and the corresponding runner-up for each World Cup year. This is the first  table under "Results". 

::: {.callout-tip collapse="true"}
### Solution

```{r}
# This is the URL we want to scrape data from
url <- "https://en.wikipedia.org/wiki/FIFA_Women%27s_World_Cup"

# This is a local HTML file to avoid scraping data from it each time I compile this quarto file. 
html_worldcup <- here::here("lectures","04-data-collection", "world-cup.html")

if(!file.exists(html_worldcup)){
  page <- read_html(url)
  write_html(page, html_worldcup)
} else {
  page <- read_html(html_worldcup)
}
```

```{r}
# Extract the table and show the first few rows
page %>%
  html_table(fill = TRUE) %>%
  .[[5]] %>% 
  head()
```

:::

- Use `rvest` to extract a table containing the number of goal scored by country. This is the second table under "Top goalscores". 

::: {.callout-tip collapse="true"}
### Solution

```{r}
# Extract the table and show the first few rows
page %>%
  html_table(fill = TRUE) %>%
  .[[9]] %>% 
  head()
```

:::

## Part 2: Extract text 

- Use `html_nodes()` to select the introductory paragraph(s) of text (e.g., the first three `<p>` tags under the main content) from the introductory section of the Wikipedia page, where key information about the World Cup  is provided. 
- Use `html_text()` to retrieve the text and clean it up if necessary.

::: {.callout-tip collapse="true"}
### Solution

```{r}
page %>%
  html_nodes("p") %>%   # Selecting the first few paragraphs
  .[1:3] %>%            # Adjust based on the number of paragraphs you want
  html_text() %>%
  paste(collapse = " ")
```

:::


## Part 3: Extract external links

Here, we will extract a list of external links related to the FIFA Womenâ€™s World Cup from the "References" section at the bottom.

- Use `html_nodes()` to select the list of references. 
- Use `html_elements()` to select all links (`<a>` tags) in the references section.
- Use `html_attr()` to extract the URLs (`href`) for these links.
- Display a list of all unique URLs.

::: {.callout-tip collapse="true"}
### Solution

```{r}
page %>%
  html_nodes(".references li") %>% 
  html_elements("a") %>%
  html_attr("href") %>%
  unique()
```

:::

## Part 4: Extract links from images

Here we will extract the URLs for images on the page, particularly images related to tournament trophies or logos.

- Use `html_nodes()` to select <img> tags.
- Use `html_attr()` to get the image source (`src`) URLs.
- Filter for images that contain keywords like "FIFA", "World Cup", or "Logo" in their src using the `grepl` function.

::: {.callout-tip collapse="true"}
### Solution

```{r}
page %>%
  html_nodes("img") %>%
  html_attr("src") %>%
  tibble(url = .) %>% 
  filter(grepl("FIFA|World_Cup|Logo", url))
```

:::



# Post-lecture

## Other good R packages to know about 

- [googlesheets4](https://cran.r-project.org/web/packages/googlesheets4/index.html) to interact with Google Sheets in R
- [googledrive](https://googledrive.tidyverse.org/) to interact with files on your Google Drive


## Additional practice

Here are some additional practice questions to help you think about the material discussed.

::: callout-note
### Questions

1. Using the GitHub API, access the repository information and ask how many open github issues you have?
2. Pick another API that we have not discussed here and use `httr` to retreive data from it. 
3. Look how many open issues there are in the `dplyr` package in the `tidyverse`. 
4. Practice requesting data from the [openFDA API](https://open.fda.gov), which returns JSON files.  This API provides create easy access to public data, to create a new level of openness and accountability, to ensure the privacy and security of public FDA data, and ultimately to educate the public and save lives. See [data definitions](https://open.fda.gov/data/datadictionary) for all included data.

:::

